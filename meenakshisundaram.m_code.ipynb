{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Loading and Preprocessing\n",
    "We'll assume you have a dataset in the form of video files, text descriptions, and speech captions. We'll need to preprocess these inputs and extract features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.models import resnet50\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load video data\n",
    "def load_video_data(directory):\n",
    "    video_paths = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.mp4')]\n",
    "    return video_paths\n",
    "\n",
    "# Load video data\n",
    "video_directory = 'C:/Users/Manikandan/Desktop/RA/final_cut/sample/sample'\n",
    "video_paths = load_video_data(video_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess video frames\n",
    "def preprocess_video(video_path, frame_count=16):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    interval = max(1, total_frames // frame_count)\n",
    "    \n",
    "    frame_idx = 0\n",
    "    while len(frames) < frame_count and cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_idx % interval == 0:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(transform(frame))\n",
    "        frame_idx += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    while len(frames) < frame_count:\n",
    "        frames.append(frames[-1])\n",
    "    \n",
    "    return torch.stack(frames)\n",
    "\n",
    "# Preprocess video frames\n",
    "video_features = torch.stack([preprocess_video(video_path) for video_path in video_paths])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load text descriptions and speech captions from CSV\n",
    "def load_text_and_speech(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    descriptions = df['creative_data_description'].tolist()\n",
    "    speech_captions = df['speech'].tolist()\n",
    "    return descriptions, speech_captions\n",
    "\n",
    "# Load text descriptions and speech captions\n",
    "descriptions, speech_captions = load_text_and_speech('C:/Users/Manikandan/Desktop/RA/final_cut/Sample.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text descriptions using BERT tokenizer\n",
    "def preprocess_text(text, tokenizer, max_length=128):\n",
    "    tokens = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length)\n",
    "    return tokens['input_ids'], tokens['attention_mask']\n",
    "\n",
    "# Preprocess text descriptions using BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text_features = [preprocess_text(desc, tokenizer) for desc in descriptions]\n",
    "text_input_ids = torch.cat([feat[0] for feat in text_features])\n",
    "text_attention_masks = torch.cat([feat[1] for feat in text_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess speech captions using BERT tokenizer\n",
    "def preprocess_speech(speech_captions):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    tokenized_captions = tokenizer(speech_captions, return_tensors='pt', padding=True, truncation=True)\n",
    "    input_ids = tokenized_captions['input_ids']\n",
    "    attention_mask = tokenized_captions['attention_mask']\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "# Preprocess speech captions using BERT tokenizer\n",
    "speech_input_ids, speech_attention_mask = preprocess_speech(speech_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manikandan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Manikandan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Define the multimodal classifier model\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, video_feature_size, text_feature_size, hidden_size, num_questions):\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "        self.video_model = resnet50(pretrained=True)\n",
    "        self.video_model.fc = nn.Linear(self.video_model.fc.in_features, video_feature_size)\n",
    "        \n",
    "        self.text_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.speech_rnn = nn.RNN(input_size=1, hidden_size=hidden_size, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(video_feature_size + text_feature_size + hidden_size, num_questions)\n",
    "\n",
    "    def forward(self, video_feat, text_feat, text_mask, speech_feat):\n",
    "        batch_size, num_frames, channels, height, width = video_feat.shape\n",
    "        video_feat = video_feat.view(-1, channels, height, width)\n",
    "        video_out = self.video_model(video_feat)\n",
    "        video_out = video_out.view(batch_size, num_frames, -1).mean(1)\n",
    "        \n",
    "        text_feat = text_feat.squeeze(1)\n",
    "        text_mask = text_mask.squeeze(1)\n",
    "        text_out = self.text_model(input_ids=text_feat.long(), attention_mask=text_mask.long())[1]\n",
    "        \n",
    "        speech_out, _ = self.speech_rnn(speech_feat.unsqueeze(-1).float())\n",
    "        speech_out = speech_out[:, -1, :]\n",
    "        \n",
    "        combined_feat = torch.cat((video_out, text_out, speech_out), dim=1)\n",
    "        \n",
    "        output = torch.sigmoid(self.fc(combined_feat))\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Initialize the multi-modal classifier model\n",
    "video_feature_size = 2048\n",
    "text_feature_size = 768\n",
    "hidden_size = 128\n",
    "num_questions = 21\n",
    "model = MultiModalClassifier(video_feature_size, text_feature_size, hidden_size, num_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated labels for each question (replace with actual labels)\n",
    "labels = torch.randint(0, 2, (150, num_questions)).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data into train, val, and test sets\n",
    "def split_data(data, labels, test_size=0.2, val_size=0.1):\n",
    "    data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=test_size, random_state=42)\n",
    "    data_train, data_val, labels_train, labels_val = train_test_split(data_train, labels_train, test_size=val_size, random_state=42)\n",
    "    return data_train, data_val, data_test, labels_train, labels_val, labels_test\n",
    "\n",
    "# Split data into train, val, and test sets\n",
    "video_feats_train, video_feats_val, video_feats_test, labels_train, labels_val, labels_test = split_data(video_features, labels)\n",
    "text_feats_train, text_feats_val, text_feats_test, _, _, _ = split_data(text_input_ids, labels)\n",
    "text_masks_train, text_masks_val, text_masks_test, _, _, _ = split_data(text_attention_masks, labels)\n",
    "speech_feats_train, speech_feats_val, speech_feats_test, _, _, _ = split_data(speech_input_ids, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create data loaders\n",
    "def create_dataloader(video_feats, text_feats, text_masks, speech_feats, labels, batch_size=1):\n",
    "    dataset = TensorDataset(video_feats, text_feats, text_masks, speech_feats, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = create_dataloader(video_feats_train, text_feats_train, text_masks_train, speech_feats_train, labels_train)\n",
    "val_loader = create_dataloader(video_feats_val, text_feats_val, text_masks_val, speech_feats_val, labels_val)\n",
    "test_loader = create_dataloader(video_feats_test, text_feats_test, text_masks_test, speech_feats_test, labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 0.9468960960706075\n",
      "Epoch 2, Validation Loss: 0.7040195514758428\n",
      "Epoch 3, Validation Loss: 0.8068806628386179\n",
      "Epoch 4, Validation Loss: 0.761273980140686\n",
      "Epoch 5, Validation Loss: 0.6988709072271982\n",
      "Epoch 6, Validation Loss: 0.7160429507493973\n",
      "Epoch 7, Validation Loss: 0.7185488790273666\n",
      "Epoch 8, Validation Loss: 0.7238199909528097\n",
      "Epoch 9, Validation Loss: 0.7111341108878454\n",
      "Epoch 10, Validation Loss: 0.704804057876269\n",
      "Epoch 11, Validation Loss: 0.7077264984448751\n",
      "Epoch 12, Validation Loss: 0.7103284895420074\n",
      "Epoch 13, Validation Loss: 0.694024403889974\n",
      "Epoch 14, Validation Loss: 0.7389432787895203\n",
      "Epoch 15, Validation Loss: 0.7033341477314631\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for video_batch, text_batch, text_mask_batch, speech_batch, label_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(video_batch, text_batch, text_mask_batch, speech_batch)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for video_batch, text_batch, text_mask_batch, speech_batch, label_batch in val_loader:\n",
    "            outputs = model(video_batch, text_batch, text_mask_batch, speech_batch)\n",
    "            val_loss += criterion(outputs, label_batch).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m all_preds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     32\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 34\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(video_paths)):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Define the questions\n",
    "questions = [\n",
    "    \"Is there a call to go online (e.g., shop online, visit the Web)? \",\n",
    "    \"Is there online contact information provided (e.g., URL, website)? \",\n",
    "    \"Is there a visual or verbal call to purchase (e.g., buy now, order now)?\",\n",
    "    \"Does the ad portray a sense of urgency to act (e.g., buy before sales ends, order before ends)? \",\n",
    "    \"Is there an incentive to buy (e.g., a discount, a coupon, a sale or \\\"limited time offer\\\")? \" ,\n",
    "    \"Is there offline contact information provided (e.g., phone, mail, store location)?\",\n",
    "    \"Is there mention of something free? \",\n",
    "    \"Does the ad mention at least one specific product or service (e.g., model, type, item)? \",\n",
    "    \"Is there any verbal or visual mention of the price?\",\n",
    "    \"Does the ad show the brand (logo, brand name) or trademark (something that most people know is the brand) multiple times?\",\n",
    "    \"Does the ad show the brand or trademark exactly once at the end of the ad?\",\n",
    "    \"Is the ad intended to affect the viewer emotionally, either with positive emotion (fun, joy), negative emotion (sad, anxious) or another type of emotion? (Note: You may not personally agree, but assess if that was the intention.)\",\n",
    "    \"Does the ad give you a positive feeling about the brand? \",\n",
    "    \"Does the ad have a story arc, with a beginning and an end? \",\n",
    "    \"Does the ad have a reversal of fortune, where something changes for the better, or changes for the worse?\",\n",
    "    \"Does the ad have relatable characters? \",\n",
    "    \"Is the ad creative/clever?\",\n",
    "    \"Is the ad intended to be funny? (Note: You may not personally agree, but assess if that was the intention.) \",\n",
    "    \"Does this ad provide sensory stimulation (e.g., cool visuals, arousing music, mouth-watering)? \",\n",
    "    \"Is the ad visually pleasing?\",\n",
    "    \"Does the ad have cute elements like animals, babies, animated, characters, etc?\"\n",
    "]\n",
    "\n",
    "# Initialize lists to store video answers, predictions, and labels\n",
    "video_answers = []\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(len(video_paths)):\n",
    "        video_feat = preprocess_video(video_paths[i]).unsqueeze(0)\n",
    "        text_feat, text_mask = preprocess_text(descriptions[i], tokenizer)\n",
    "        text_feat = text_feat.unsqueeze(0)\n",
    "        text_mask = text_mask.unsqueeze(0)\n",
    "        speech_feat = speech_input_ids[i].unsqueeze(0)\n",
    "        \n",
    "        output = model(video_feat, text_feat, text_mask, speech_feat)\n",
    "        \n",
    "        answers = [( int(output[0][j].item() > 0.5)) for j in range(len(questions))]\n",
    "        video_answers.append(answers)\n",
    "        \n",
    "        all_preds.append((output[0] > 0.5).int().tolist())\n",
    "        all_labels.append(labels[i].tolist())\n",
    "\n",
    "# Print answers for all videos\n",
    "for i in range(len(video_answers)):\n",
    "    print(f\"Video {i+1} answers:\")\n",
    "    for  answer in video_answers[i]:\n",
    "        answer_str = \"yes\" if answer == 1 else \"no\"\n",
    "        print(f\" {answer_str}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming `video_answers` is a list of lists containing the binary answers for each video\n",
    "# Convert the binary answers (1/0) to 'yes'/'no'\n",
    "video_answers_str = [['yes' if answer == 1 else 'no' for answer in video] for video in video_answers]\n",
    "\n",
    "# Create a DataFrame from the list of lists\n",
    "df = pd.DataFrame(video_answers_str)\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "df.to_excel('video_answers.xlsx', index=False, header=False)\n",
    "\n",
    "print(\"Video answers have been written to video_answers.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\4124882329.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ground_truth_agg = ground_truth_df.groupby('creative_data_id').apply(lambda x: x.iloc[0]).reset_index(drop=True)\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\4124882329.py:30: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  comparison_df.replace({\"Yes\": 1, \"No\": 0}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Load ground truth data\n",
    "ground_truth_df = pd.read_csv(\"C:/Users/Manikandan/Desktop/RA/final_cut/ground-truth.xlsx - Form Responses 1.csv\")\n",
    "\n",
    "# Extract relevant columns including 'creative_data_id'\n",
    "ground_truth_df = ground_truth_df[['creative_data_id'] + questions]\n",
    "\n",
    "# Define a function to take the majority vote for each question\n",
    "def majority_vote(df):\n",
    "    return df.mode().iloc[0]\n",
    "\n",
    "# Group by creative_data_id and apply majority vote\n",
    "ground_truth_agg = ground_truth_df.groupby('creative_data_id').apply(lambda x: x.iloc[0]).reset_index(drop=True)\n",
    "ground_truth_agg = ground_truth_agg.drop('creative_data_id', axis=1)\n",
    "\n",
    "# Prepare predictions DataFrame\n",
    "predictions_df = pd.DataFrame(video_answers, columns= questions)\n",
    "\n",
    "# Ensure the data types are consistent\n",
    "predictions_df = predictions_df.astype(str)\n",
    "ground_truth_agg = ground_truth_agg.astype(str)\n",
    "\n",
    "# Merge predictions with ground truth on creative_data_id\n",
    "comparison_df = pd.concat([predictions_df, ground_truth_agg.reset_index(drop=True)], axis=1, keys=['pred', 'true'])\n",
    "comparison_df.columns = [f\"{col}_pred\" if idx < len(predictions_df.columns) else f\"{col}_true\" for idx, col in enumerate(comparison_df.columns)]\n",
    "#print(comparison_df)\n",
    "comparison_df.replace({\"Yes\": 1, \"No\": 0}, inplace=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Is there a call to go online (e.g., shop online, visit the Web)? :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
      "c:\\Users\\Manikandan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
      "C:\\Users\\Manikandan\\AppData\\Local\\Temp\\ipykernel_14892\\2661709833.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Precision: 1.0\n",
      "  Recall: 0.013888888888888888\n",
      "  F1 Score: 0.0273972602739726\n",
      "\n",
      "Metrics for Is there online contact information provided (e.g., URL, website)? :\n",
      "  Precision: 0.45217391304347826\n",
      "  Recall: 0.7222222222222222\n",
      "  F1 Score: 0.5561497326203209\n",
      "\n",
      "Metrics for Is there a visual or verbal call to purchase (e.g., buy now, order now)?:\n",
      "  Precision: 0.5\n",
      "  Recall: 0.5076923076923077\n",
      "  F1 Score: 0.5038167938931297\n",
      "\n",
      "Metrics for Does the ad portray a sense of urgency to act (e.g., buy before sales ends, order before ends)? :\n",
      "  Precision: 0.225\n",
      "  Recall: 0.20454545454545456\n",
      "  F1 Score: 0.21428571428571427\n",
      "\n",
      "Metrics for Is there an incentive to buy (e.g., a discount, a coupon, a sale or \"limited time offer\")? :\n",
      "  Precision: 0.38095238095238093\n",
      "  Recall: 0.49230769230769234\n",
      "  F1 Score: 0.42953020134228187\n",
      "\n",
      "Metrics for Is there offline contact information provided (e.g., phone, mail, store location)?:\n",
      "  Precision: 0.25\n",
      "  Recall: 0.6\n",
      "  F1 Score: 0.35294117647058826\n",
      "\n",
      "Metrics for Is there mention of something free? :\n",
      "  Precision: 0.029411764705882353\n",
      "  Recall: 0.14285714285714285\n",
      "  F1 Score: 0.04878048780487805\n",
      "\n",
      "Metrics for Does the ad mention at least one specific product or service (e.g., model, type, item)? :\n",
      "  Precision: 0.835820895522388\n",
      "  Recall: 0.48695652173913045\n",
      "  F1 Score: 0.6153846153846154\n",
      "\n",
      "Error in question:  Is there any verbal or visual mention of the price?\n",
      "\n",
      "Metrics for Does the ad show the brand (logo, brand name) or trademark (something that most people know is the brand) multiple times?:\n",
      "  Precision: 0.8\n",
      "  Recall: 0.24561403508771928\n",
      "  F1 Score: 0.37583892617449666\n",
      "\n",
      "Metrics for Does the ad show the brand or trademark exactly once at the end of the ad?:\n",
      "  Precision: 0.7101449275362319\n",
      "  Recall: 0.45794392523364486\n",
      "  F1 Score: 0.5568181818181818\n",
      "\n",
      "Metrics for Is the ad intended to affect the viewer emotionally, either with positive emotion (fun, joy), negative emotion (sad, anxious) or another type of emotion? (Note: You may not personally agree, but assess if that was the intention.):\n",
      "  Precision: 0.7910447761194029\n",
      "  Recall: 0.4608695652173913\n",
      "  F1 Score: 0.5824175824175825\n",
      "\n",
      "Metrics for Does the ad give you a positive feeling about the brand? :\n",
      "  Precision: 0.808695652173913\n",
      "  Recall: 0.768595041322314\n",
      "  F1 Score: 0.788135593220339\n",
      "\n",
      "Metrics for Does the ad have a story arc, with a beginning and an end? :\n",
      "  Precision: 0.33962264150943394\n",
      "  Recall: 0.45\n",
      "  F1 Score: 0.3870967741935484\n",
      "\n",
      "Metrics for Does the ad have a reversal of fortune, where something changes for the better, or changes for the worse?:\n",
      "  Precision: 0.21153846153846154\n",
      "  Recall: 0.44\n",
      "  F1 Score: 0.2857142857142857\n",
      "\n",
      "Metrics for Does the ad have relatable characters? :\n",
      "  Precision: 0.5952380952380952\n",
      "  Recall: 0.5813953488372093\n",
      "  F1 Score: 0.5882352941176471\n",
      "\n",
      "Metrics for Is the ad creative/clever?:\n",
      "  Precision: 0.0\n",
      "  Recall: 0.0\n",
      "  F1 Score: 0.0\n",
      "\n",
      "Metrics for Is the ad intended to be funny? (Note: You may not personally agree, but assess if that was the intention.) :\n",
      "  Precision: 0.23809523809523808\n",
      "  Recall: 0.6666666666666666\n",
      "  F1 Score: 0.3508771929824561\n",
      "\n",
      "Metrics for Does this ad provide sensory stimulation (e.g., cool visuals, arousing music, mouth-watering)? :\n",
      "  Precision: 0.5142857142857142\n",
      "  Recall: 0.20689655172413793\n",
      "  F1 Score: 0.29508196721311475\n",
      "\n",
      "Metrics for Is the ad visually pleasing?:\n",
      "  Precision: 0.6216216216216216\n",
      "  Recall: 0.989247311827957\n",
      "  F1 Score: 0.7634854771784232\n",
      "\n",
      "Metrics for Does the ad have cute elements like animals, babies, animated, characters, etc?:\n",
      "  Precision: 0.0\n",
      "  Recall: 0.0\n",
      "  F1 Score: 0.0\n",
      "\n",
      "Overall Agreement Percentage: 53.46666666666666%\n",
      "Overall Precision: 0.5314762375291437\n",
      "Overall Recall: 0.5306675232138696\n",
      "Overall F1 Score: 0.5291749634402335\n"
     ]
    }
   ],
   "source": [
    "#comparison_df = comparison_df.astype(int)\n",
    "\n",
    "# Initialize lists to store all predictions and ground truth labels\n",
    "all_preds_flat = []\n",
    "all_labels_flat = []\n",
    "\n",
    "# Loop through each question to calculate metrics\n",
    "for question in questions:\n",
    "    preds = comparison_df[f\"('pred', '{question}')_pred\"]\n",
    "    true = comparison_df[f\"('true', '{question}')_true\"]\n",
    "    preds.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
    "    true.replace({\"1\": 1,\"Yes, visual\": 1,\"Yes, both\": 1, \"0\": 0}, inplace=True)\n",
    "\n",
    "    flag=False\n",
    "    for i in preds:\n",
    "        if i not in [1,0]:\n",
    "            flag=True\n",
    "    \n",
    "    for i in true:\n",
    "        if i not in [1,0]:\n",
    "            flag=True\n",
    "\n",
    "    if flag:\n",
    "        print(\"Error in question: \", question)\n",
    "        print(\"\")\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    all_preds_flat.extend(preds)\n",
    "    all_labels_flat.extend(true)\n",
    "\n",
    "    # Calculate precision, recall, and F1 score for the current question\n",
    "    precision = precision_score(true, preds, pos_label=1, average='binary')\n",
    "    recall = recall_score(true, preds, pos_label=1, average='binary')\n",
    "    f1 = f1_score(true, preds, pos_label=1, average='binary')\n",
    "    \n",
    "    print(f\"Metrics for {question}:\")\n",
    "    print(f\"  Precision: {precision}\")\n",
    "    print(f\"  Recall: {recall}\")\n",
    "    print(f\"  F1 Score: {f1}\\n\")\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_precision = precision_score(all_labels_flat, all_preds_flat, average='macro')\n",
    "overall_recall = recall_score(all_labels_flat, all_preds_flat, average='macro')\n",
    "overall_f1 = f1_score(all_labels_flat, all_preds_flat, average='macro')\n",
    "\n",
    "agreement_percentage = sum([pred == label for pred, label in zip(all_preds_flat, all_labels_flat)]) / len(all_preds_flat) * 100\n",
    "\n",
    "print(f\"Overall Agreement Percentage: {agreement_percentage}%\")\n",
    "print(f\"Overall Precision: {overall_precision}\")\n",
    "print(f\"Overall Recall: {overall_recall}\")\n",
    "print(f\"Overall F1 Score: {overall_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
